{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from stitching.images import Images\n",
    "from stitching.feature_detector import FeatureDetector\n",
    "from stitching.feature_matcher import FeatureMatcher\n",
    "from stitching.subsetter import Subsetter\n",
    "from stitching.camera_estimator import CameraEstimator\n",
    "from stitching.camera_adjuster import CameraAdjuster\n",
    "from stitching.camera_wave_corrector import WaveCorrector\n",
    "from stitching.warper import Warper\n",
    "from stitching.timelapser import Timelapser\n",
    "from stitching.cropper import Cropper\n",
    "from stitching.seam_finder import SeamFinder\n",
    "from stitching.exposure_error_compensator import ExposureErrorCompensator\n",
    "from stitching.blender import Blender\n",
    "from stitching import Stitcher\n",
    "from stitching import AffineStitcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow displaying resulting images within notebook and load the correct img paths to used image sets\n",
    "\n",
    "def plot_image(img, figsize_in_inches=(5,5)):\n",
    "    fig, ax = plt.subplots(figsize=figsize_in_inches)\n",
    "    ax.imshow(cv.cvtColor(img, cv.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "    \n",
    "def plot_images(imgs, figsize_in_inches=(5,5)):\n",
    "    fig, axs = plt.subplots(1, len(imgs), figsize=figsize_in_inches)\n",
    "    for col, img in enumerate(imgs):\n",
    "        axs[col].imshow(cv.cvtColor(img, cv.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "\n",
    "def get_image_paths(img_set):\n",
    "    return [str(path.relative_to('.')) for path in Path('images').rglob(f'{img_set}*')]\n",
    "\n",
    "imgs = get_image_paths(\"IMG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize Images\n",
    "\n",
    "The first step is to resize the images to medium resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = Images.of(imgs)\n",
    "medium_imgs = list(images.resize(Images.Resolution.MEDIUM))\n",
    "low_imgs = list(images.resize(Images.Resolution.LOW))\n",
    "final_imgs = list(images.resize(Images.Resolution.FINAL))\n",
    "\n",
    "plot_images(low_imgs, (20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_size = images.sizes[0]\n",
    "medium_size = images.get_image_size(medium_imgs[0])\n",
    "low_size = images.get_image_size(low_imgs[0])\n",
    "final_size = images.get_image_size(final_imgs[0])\n",
    "\n",
    "print(f\"Original Size: {original_size}  -> {'{:,}'.format(np.prod(original_size))} px ~ 1 MP\")\n",
    "print(f\"Medium Size:   {medium_size}  -> {'{:,}'.format(np.prod(medium_size))} px ~ 0.6 MP\")\n",
    "print(f\"Low Size:      {low_size}   -> {'{:,}'.format(np.prod(low_size))} px ~ 0.1 MP\")\n",
    "print(f\"Final Size:    {final_size}  -> {'{:,}'.format(np.prod(final_size))} px ~ 1 MP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Features\n",
    "\n",
    "For medium-sized images, our goal is to identify features that can characterize prominent elements present in the images. We will do this using the `FeatureDetector` class, with the intention of identifying similar features in other images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder = FeatureDetector()\n",
    "features = [finder.detect_features(img) for img in medium_imgs]\n",
    "keypoints_center_img = finder.draw_keypoints(medium_imgs[1], features[1])\n",
    "plot_image(keypoints_center_img, (15,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match Features\n",
    "\n",
    "We can now compare the features of the paired images using the `FeatureMatcher` class. During this process, we examine confidences, computed as \n",
    "\n",
    "`confidence = number of inliers / (8 + 0.3 * number of matches)`\n",
    "\n",
    "and inliers, determined through the (RANSAC) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From this, we can see which images have a low and high matching confidence with the other images. \n",
    "matcher = FeatureMatcher()\n",
    "matches = matcher.match_features(features)\n",
    "matcher.get_confidence_matrix(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_relevant_matches = matcher.draw_matches_matrix(medium_imgs, features, matches, conf_thresh=1, \n",
    "                                                   inliers=True, matchColor=(0, 255, 0))\n",
    "for idx1, idx2, img in all_relevant_matches:\n",
    "    print(f\"Matches Image {idx1+1} to Image {idx2+1}\")\n",
    "    plot_image(img, (20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset\n",
    "\n",
    "As shown above, the noise image lacks any connection to the other images constituting the panorama. Our next step involves crafting a subset that includes only the pertinent images. To accomplish this, we employ the `Subsetter` class and define a `confidence_threshold` to determine when a match is considered a good match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsetter = Subsetter()\n",
    "dot_notation = subsetter.get_matches_graph(images.names, matches)\n",
    "print(dot_notation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These matches graph visualizes what we've saw in the confidence matrix. Now, we want to subset all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = subsetter.get_indices_to_keep(features, matches)\n",
    "medium_imgs = subsetter.subset_list(medium_imgs, indices)\n",
    "low_imgs = subsetter.subset_list(low_imgs, indices)\n",
    "final_imgs = subsetter.subset_list(final_imgs, indices)\n",
    "features = subsetter.subset_list(features, indices)\n",
    "matches = subsetter.subset_matches(matches, indices)\n",
    "images.subset(indices)\n",
    "print(images.names)\n",
    "print(matcher.get_confidence_matrix(matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera Estimation, Adjustion and Correction\n",
    "\n",
    "Our next objective involves calibrating cameras for the purpose of warping images, ensuring their correct composition. This process employs the `CameraEstimator`, `CameraAdjuster`, and `WaveCorrector` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_estimator = CameraEstimator()\n",
    "camera_adjuster = CameraAdjuster()\n",
    "wave_corrector = WaveCorrector()\n",
    "cameras = camera_estimator.estimate(features, matches)\n",
    "cameras = camera_adjuster.adjust(features, matches, cameras)\n",
    "cameras = wave_corrector.correct(cameras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warp Images\n",
    "\n",
    "We now want to warp the images itself into the final plane, using the `Warper` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warper = Warper()\n",
    "\n",
    "# first set the medium focal length of the cameras as scale\n",
    "warper.set_scale(cameras)\n",
    "\n",
    "# warp low resolution images\n",
    "low_sizes = images.get_scaled_img_sizes(Images.Resolution.LOW)\n",
    "camera_aspect = images.get_ratio(Images.Resolution.MEDIUM, Images.Resolution.LOW)  \n",
    "warped_low_imgs = list(warper.warp_images(low_imgs, cameras, camera_aspect))\n",
    "warped_low_masks = list(warper.create_and_warp_masks(low_sizes, cameras, camera_aspect))\n",
    "low_corners, low_sizes = warper.warp_rois(low_sizes, cameras, camera_aspect)\n",
    "\n",
    "# warp final resolution images\n",
    "final_sizes = images.get_scaled_img_sizes(Images.Resolution.FINAL)\n",
    "camera_aspect = images.get_ratio(Images.Resolution.MEDIUM, Images.Resolution.FINAL)\n",
    "warped_final_imgs = list(warper.warp_images(final_imgs, cameras, camera_aspect))\n",
    "warped_final_masks = list(warper.create_and_warp_masks(final_sizes, cameras, camera_aspect))\n",
    "final_corners, final_sizes = warper.warp_rois(final_sizes, cameras, camera_aspect)\n",
    "\n",
    "# plot results\n",
    "plot_images(warped_low_imgs, (10,10))\n",
    "plot_images(warped_low_masks, (10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_corners)\n",
    "print(final_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crop\n",
    "\n",
    "No individual image encompasses the entire height of the final plane. To generate a panorama devoid of black borders, we can proceed to calculate the largest common interior rectangle and subsequently crop the individual images accordingly, utilizing the functionalities of the `Cropper` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropper = Cropper()\n",
    "\n",
    "# estimate panorama mask of potential final panorama using a Blender\n",
    "mask = cropper.estimate_panorama_mask(warped_low_imgs, warped_low_masks, low_corners, low_sizes)\n",
    "plot_image(mask, (5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lir = cropper.estimate_largest_interior_rectangle(mask)\n",
    "print(lir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = lir.draw_on(mask, size=2)\n",
    "plot_image(plot, (5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by zero centering the warped corners, the rectangle of the images within the final plane can be determined\n",
    "low_corners = cropper.get_zero_center_corners(low_corners)\n",
    "rectangles = cropper.get_rectangles(low_corners, low_sizes)\n",
    "plot = rectangles[1].draw_on(plot, (0, 255, 0), 2)  # The rectangle of the center img\n",
    "plot_image(plot, (5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the overlap, the new corners and sizes can be determined \n",
    "overlap = cropper.get_overlap(rectangles[1], lir)\n",
    "plot = overlap.draw_on(plot, (255, 0, 0), 2)\n",
    "plot_image(plot, (5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are able to crop it now using the blue rectangle in the coordinate system of the original green image\n",
    "intersection = cropper.get_intersection(rectangles[1], overlap)\n",
    "plot = intersection.draw_on(warped_low_masks[1], (255, 0, 0), 2)\n",
    "plot_image(plot, (2.5,2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now crop the images and masks and obtain new corners and sizes using all this information \n",
    "cropper.prepare(warped_low_imgs, warped_low_masks, low_corners, low_sizes)\n",
    "cropped_low_masks = list(cropper.crop_images(warped_low_masks))\n",
    "cropped_low_imgs = list(cropper.crop_images(warped_low_imgs))\n",
    "low_corners, low_sizes = cropper.crop_rois(low_corners, low_sizes)\n",
    "lir_aspect = images.get_ratio(Images.Resolution.LOW, Images.Resolution.FINAL)  \n",
    "cropped_final_masks = list(cropper.crop_images(warped_final_masks, lir_aspect))\n",
    "cropped_final_imgs = list(cropper.crop_images(warped_final_imgs, lir_aspect))\n",
    "final_corners, final_sizes = cropper.crop_rois(final_corners, final_sizes, lir_aspect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seam Masks\n",
    "\n",
    "Seam masks identify a transition line between images with minimal disruption, leveraging the capabilities of the `SeamFinder` class. The seams are derived from the warped low-resolution images and subsequently resized to match the resolution of the final warped images. These seam masks play a crucial role in the blending step, providing guidance on how the images should be seamlessly composed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seam_finder = SeamFinder()\n",
    "seam_masks = seam_finder.find(cropped_low_imgs, low_corners, cropped_low_masks)\n",
    "seam_masks = [seam_finder.resize(seam_mask, mask) for seam_mask, mask in zip(seam_masks, cropped_final_masks)]\n",
    "seam_masks_plots = [SeamFinder.draw_seam_mask(img, seam_mask) for img, seam_mask in zip(cropped_final_imgs, seam_masks)]\n",
    "plot_images(seam_masks_plots, (15,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exposure Error Compensation\n",
    "\n",
    "Discrepancies in exposure errors across images can result in artifacts in the eventual panorama. These exposure errors are evaluated on the warped low-resolution images and subsequently applied to the warped final-resolution images, employing the functionalities of the `ExposureErrorCompensator` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compensator = ExposureErrorCompensator()\n",
    "compensator.feed(low_corners, cropped_low_imgs, cropped_low_masks)\n",
    "compensated_imgs = [compensator.apply(idx, corner, img, mask) \n",
    "                    for idx, (img, mask, corner) \n",
    "                    in enumerate(zip(cropped_final_imgs, cropped_final_masks, final_corners))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blending\n",
    "\n",
    "The images can be seamlessly blended into a complete panorama through the utilization of the `Blender` class. The blend strength parameter determines the extent to which images should overlay along the transitions of the masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blender = Blender()\n",
    "blender.prepare(final_corners, final_sizes)\n",
    "for img, mask, corner in zip(compensated_imgs, seam_masks, final_corners):\n",
    "    blender.feed(img, mask, corner)\n",
    "panorama, _ = blender.blend()\n",
    "plot_image(panorama, (20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the capability to illustrate the seams on the final panorama by plotting them as lines or polygons. This visualization allows us to discern which section of the panorama corresponds to each individual image. The approach involves blending single-colored dummy images with the acquired seam masks and adhering to the dimensions of the panorama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blended_seam_masks = seam_finder.blend_seam_masks(seam_masks, final_corners, final_sizes)\n",
    "plot_image(blended_seam_masks, (5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this blend can be converted into lines or weighted on top of resulting panorama\n",
    "plot_image(seam_finder.draw_seam_lines(panorama, blended_seam_masks, linesize=3), (15,10))\n",
    "plot_image(seam_finder.draw_seam_polygons(panorama, blended_seam_masks), (15,10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
